---
title: "Homework-01"
author: |
  |
  |
  | Group 19 - Le superchicche
  |
  | Cammarota Sara, Iadisernia Giulia, Petrucci Ilaria
  |
date:  |
  | May 15th, 2023
output:
  html_document:
    keep_md: yes
    theme: united
    css: styles.css
urlcolor: magenta
linkcolor: cyan
geometry: margin=2cm
fontsize: 12pt
---

<style>
p {
  text-align: justify;
}
</style>

\

```{r, fig.align='center', out.width='400px', out.height='200px', echo=FALSE}
knitr::include_graphics("intro4.jpg")
```

\

# Part I

### 1.1 "Be linear in transformed feature space"

**Splines** are a powerful and flexible method that is able to capture complex structures in the data by fitting together a series of polynomial segments which are smoothly connected at specified points called knots. In splines we estimate our target function as 

$$
\hat{f}(x)= \sum_{i = 1}^{N} \beta_i \, g_i(x)
$$

for some suitable (to be determined) values of $\beta_i$, where $g_i(x)$ are the truncated power functions and $N = d + q + 1$ is the number of parameters. In particular $d$ is the degree of the spline polynomial and $q$ is the number of knots. 

The splines approach is a manifestation of _being linear in transformed feature space_ because we perform an embedding of the features into a more complex space where we can perform linear regression; in our case, for example, we will start with a simple vector $x$ ($1$-dimensional) and we will obtain a large matrix with $d+q+1$ columns, hence $d+q+1$ features.

We basically transform the linear space into a higher-dimensional space and look for a *linear-in-higher-dimensional-space* relation that links the input to the output, and finally project this relation back into the original feature space. 

On the other hand, **orthogonal series expansion** typically involves using a fixed set of basis functions, such as the cosine basis functions $\phi_j(x) = \sqrt2\,cos\,(j\pi x)\,$, to represent the data, which can be limiting in terms of the shapes and patterns that can be captured. 

The main difference between splines and the orthogonal series expansion is that the latter allows us to build a *global* object that fits the data in the *whole* feature space, while the power of the former method, as already stated, is that it is the union of several *local* objects. In this sense we *may* be able to capture more complex and hidden structures in the data. 

Another important difference is in the way the basis functions are constructed. Orthogonal series expansion uses a fixed set of basis functions that are *orthogonal* to each other, which can simplify the computation of the coefficients for the expansion. Splines, on the other hand, typically use basis functions that are not orthogonal - i.e. truncated power basis - , but are chosen to be locally supported and smooth, which can lead to more accurate approximation of the data.

### 1.2 Truncated power basis functions

_Truncated power basis functions_ $G_{d, q}$ are defined as
$$\{g_1(x) = 1, g_2(x) = x, ..., g_{d+1}(x) = x^d\}$$ and $$\{g_{(d+1)+j}(x) = (x - \xi_j)_+^d \}_{j=1}^q \ \ \text{where} (x)_+ = max\{0, x\}$$

We choose _degree_ $d \in \{1, 3\}$ and the number of equi-spaced _knots_ $q \in \{3, 10\}$ where $\{\xi_j\}_{j=1}^q \in (0, 1)$.

### 1.2.1 Our implementation vs...

Let's suppose $d=1$ and $q=3$

$\{g_i(x)\}_{i=1}^2 = \{g_1(x) = 1, g_2(x) = x\}$ and $\{g_{2+j}(x)\}_{j=1}^3 = \{g_3(x) = (x - \xi_1)_+, g_4(x) = (x - \xi_2)_+, g_5(x) = (x - \xi_3)_+\}$ where the knots $\{\xi_j\}_{j=1}^3$ are respectively equal to $0.25, 0.50, 0.75$.

```{r basis d equal to 1 q equal to 3, echo=FALSE, fig.align= 'center'}

d <- 1
q <- 3
knots <- seq(1/(q+1), 1 - 1/(q+1), 1/(q+1))

g2 <- function(x) x
g3 <- function(x) pmax((x - knots[1]), 0)
g4 <- function(x) pmax((x - knots[2]), 0)
g5 <- function(x) pmax((x - knots[3]), 0)
curve(g2(x), col = "orchid", lwd = 2,
      main = "Truncated power basis with d=1 & q=3", ylab = expression(g[i](x)))
curve(g3(x), add = T, col = "purple", lwd = 2)
curve(g4(x), add = T, col = "blue", lwd = 2)
curve(g5(x), add = T, col = "darkblue", lwd = 2)
legend("topleft", legend = c(expression(g[2](x)),
                             expression(g[3](x)),
                             expression(g[4](x)),
                             expression(g[5](x))),
       col = c("orchid", "purple", "blue", "darkblue"), lwd = 3, cex=1)
abline(v = knots, lty = 2)
text(0.32, 1, expression(xi[1] == 0.25), cex = 0.8)
text(0.56, 1, expression(xi[2] == 0.50), cex = 0.8)
text(0.82, 1, expression(xi[3] == 0.75), cex = 0.8)


```

When $d=1$ and $q=10$

$$\{g_i(x)\}_{i=1}^2 = \{g_1(x) = 1, g_2(x) = x\} \ \ \text{and} \ \ \{g_{2+j}(x)\}_{j=1}^{10}$$ where the knots $\{\xi_j\}_{j=1}^3$ are respectively equal to $0.09, 0.18, 0.27, 0.36, 0.45, 0.54, 0.63, 0.72, 0.81, 0.91$. In the figure below we plot some of these basis functions.

```{r basis d equal to 1 and q equal to 10, echo=FALSE, fig.align= 'center'}

q <- 10

curves <- list()
knots <- seq(1/(q+1), 1 - 1/(q+1), 1/(q+1))
j = seq(1, q, 2)
for(i in 1:5){
  curves[[i]] <- function(x, k = knots) pmax((x - k[j[i]]), 0)
}

plot(NULL, xlim = c(0, 1), ylim = c(0, 1), xlab = 'x', ylab = expression(g[i](x)), main = "Truncated power basis with d=1 & q=10")
cols <- c("orchid", "purple", "blue", "darkblue", 5)
for (i in 1:5) {
  curve(curves[[i]](x), add = TRUE, col = cols[[i]], lwd = 2)
  abline(v = knots[j[i]], lty = 2)
}
legend("topleft", legend = c(expression(g[3](x)),
                             expression(g[5](x)),
                             expression(g[7](x)),
                             expression(g[9](x)),
                             expression(g[11](x))),
       col = c("orchid", "purple", "blue", "darkblue", 5), lwd = 3, cex=1, bg = 'white')
text(0.03, 0.05, expression(xi[1] == 0.09), cex = 0.8)
text(0.34, 1, expression(xi[3] == 0.27), cex = 0.8)
text(0.52, 1, expression(xi[5] == 0.45), cex = 0.8)
text(0.70, 1, expression(xi[7] == 0.63), cex = 0.8)
text(0.88, 1, expression(xi[9] == 0.81), cex = 0.8)

```
Now, let's have a look at what changes when we increase the degree to $d=3$. For simplicity, let's choose $q=3$.

```{r basis d equal to 3 and q equal to 3, echo=FALSE, fig.align= 'center'}

q <- 3
d <- 3

knots <- seq(1/(q+1), 1 - 1/(q+1), 1/(q+1))

plot(NULL, xlim = c(0, 1), ylim = c(0, 1), xlab = 'x', ylab = expression(g[i](x)), main = "Truncated power basis with d=3 & q=3")
cols <- c("orchid", "purple", "blue")
curves <- list()
for(i in 1:d){
  curves[[i]] <- function(x) x**i
}
for (i in 1:length(curves)) {
  curve(curves[[i]](x), add = TRUE, col = cols[[i]], lwd = 2)
  abline(v = knots[i], lty = 2)
}


for(i in 1:length(knots)){
  curves[[i]] <- function(x, k = knots) pmax((x - k[i]), 0)**d
}
cols <- c( "darkblue", "cornflowerblue", 5)
for (i in 1:length(curves)) {
  curve(curves[[i]](x), add = TRUE, col = cols[[i]], lwd = 2)
  abline(v = knots[i], lty = 2)
}

legend("topleft", legend = c(expression(g[1](x)),
                             expression(g[2](x)),
                             expression(g[3](x)),
                             expression(g[4](x)),
                             expression(g[5](x)),
                             expression(g[6](x))),
       col = c("orchid", "purple", "blue", "darkblue", "cornflowerblue", 5), lwd = 3, cex=1, bg = 'white')
text(0.32, 1, expression(xi[1] == 0.25), cex = 0.8)
text(0.56, 1, expression(xi[2] == 0.50), cex = 0.8)
text(0.82, 1, expression(xi[3] == 0.75), cex = 0.8)

```

### 1.2.2 ...**ChatGPT**'s reply (purely copy and paste)

#### Version 1

We asked **ChatGPT** to perform the task of computing and plot the truncated power basis $\textit{G}_{d,q}$. We started with a general version of the task: 
"Can you create and plot the truncated power function in R?"\

The _answer_ was: "Certainly! I can guide you on how to create and plot a truncated power function in R. The truncated power function is defined as:\

f(x) = (x^a) / (1 + x^a)\

where 'a' is the exponent parameter. To create and plot this function, you can follow the steps below:"

```{r}
#install.packages("ggplot2")  # If not already installed
library(ggplot2)
truncated_power <- function(x, a) {
  (x^a) / (1 + x^a)
}
x <- seq(0.01, 10, by = 0.01)  # Generate x-values from 0.01 to 10
a <- 2  # Exponent parameter
y <- truncated_power(x, a)  # Calculate y-values
data <- data.frame(x = x, y = y)

```

And returned the following plot:\

```{r, echo=FALSE, fig.align='center'}
ggplot(data, aes(x = x, y = y)) +
  geom_line() +
  labs(x = "x", y = "f(x)") +
  ggtitle("Truncated Power Function") +
  theme_minimal()
```

#### Version 2

Then we experimented with a more specific version of the same question
"Can you implement the truncated power basis function and plot it in R with degree equal to 1 and number of knots equal to 3?"\

The _answer_ was:
"Certainly! The truncated power basis function is a commonly used basis function in regression analysis. It is defined as a piecewise function that consists of power functions with different truncation points (knots).\

To implement and plot the truncated power basis function in R with a degree of 1 and 3 knots, you can use the `bs()` function from the splines package. Here's an example code:"\

```{r}

# Install and load the required package
# install.packages("splines")
library(splines)

# Define the number of knots and degree
num_knots <- 3
degree <- 1

# Generate a sequence of x values
x <- seq(0, 1, length = 100)

# Create the truncated power basis function
basis <- bs(x, knots = quantile(x, probs = seq(0, 1, length = num_knots + 2)), degree = degree, intercept = TRUE)

```

Also the code for the plot was added:\

```{r, echo=FALSE,  fig.align='center'}

# Plot the basis function
plot(x, basis[, 1], type = "l", xlab = "x", ylab = "Basis Function", main = "Truncated Power Basis Function (Degree = 1, Knots = 3)")

# Add lines for the remaining basis functions
for (i in 2:(degree + 1)) {
  lines(x, basis[, i], col = i)
}

# Add a legend
legend("topright", legend = c("Basis 1", paste0("Basis ", 2:(degree + 1))), col = 1:(degree + 1), lty = 1)
```

#### Version 3

Finally, we asked ChatGPT to perform the same task as before but without using any specific `R` packages, changing also the degree and number of knots, with the following question:
"Can you implement the truncated power basis function and plot it in R with degree equal to 3 and number of knots equal to 10 without using packets?"\

The _answer_ was:
"Certainly! To implement the truncated power basis function and plot it in R without using external packages, you can create a custom function that calculates the basis function values for given input values. Here's an example code:"\

```{r}

# Define the number of knots and degree
num_knots <- 10
degree <- 3

# Generate a sequence of x values
x <- seq(0, 1, length = 100)

# Function to calculate truncated power basis function values
truncated_power_basis <- function(x, knots, degree) {
  basis <- matrix(0, nrow = length(x), ncol = degree + 1)
  basis[, 1] <- 1
  
  for (i in 2:(degree + 1)) {
    basis[, i] <- pmax(0, x - knots[i - 1])^degree
    basis[, i] <- basis[, i] - pmax(0, x - knots[num_knots])^degree
    basis[, i] <- basis[, i] / (knots[num_knots] - knots[i - 1])
  }
  
  return(basis)
}

# Generate the knots
knots <- seq(0, 1, length = num_knots + 2)

# Calculate the basis function values
basis <- truncated_power_basis(x, knots, degree)

```

```{r, echo=FALSE, fig.align='center'}

# Plot the basis function
plot(x, basis[, 1], type = "l", xlab = "x", ylab = "Basis Function", main = "Truncated Power Basis Function (Degree = 3, Knots = 10)")

# Add lines for the remaining basis functions
for (i in 2:(degree + 1)) {
  lines(x, basis[, i], col = i)
}

# Add a legend
legend("topright", legend = c("Basis 1", paste0("Basis ", 2:(degree + 1))), col = 1:(degree + 1), lty = 1)

```

'This code will generate a plot of the truncated power basis function with a degree of 3 and 10 knots. The x-axis represents the input values, and the y-axis represents the value of the basis function at each x value. The plot will show multiple lines, each corresponding to a different basis function.\

Note that in this implementation, we calculate the basis function values manually using the truncated power basis formula. The `truncated_power_basis()` function takes the input values `x`, the knots, and the degree as arguments, and returns a matrix containing the basis function values for each input value.'\

From the various tests performed, it is evident that the **ChatGPT** bot does not always reply with correct information and is dependent on the way the question is asked. For instance, look at the huge difference between the first answer, which was a very general reply on truncated power basis functions, and the others where a more specific question was provided. Therefore we can deduce that, in order to exploit this medium properly, it is necessary that we ask very specific questions which are already directed towards the desired answer, while being alert to the presence of errors (we cannot just fully trust ChatGPT!). Moreover, it is evident that the plots are not as detailed as they should be (when coded by humans), and that what Chat' returns can only be used as a basis for asking other questions or as an input for seeking help from more reliable resources. We are not saying that ChatGPT is totally useless as it still may provide some useful hints for: reasoning when starting completely from scratch, understanding how to use certain libraries, times when getting an answer in a very short time is preferable over quality content. All questions/answers that can allow us to get a better idea on how to proceed (or how not to proceed...).

Then we asked Chat GPT an opinion about it and eventually it confirmed what we stated above:
"What do you think of the service you provide?"\

This was "its" reply:\

"As an AI language model, I don't have personal opinions or subjective experiences. However, I can provide information and assistance on a wide range of topics. My purpose is to assist users in generating human-like text based on the input provided to me. The effectiveness and usefulness of the service I provide can vary depending on the specific use case and the quality of the input given to me. It's important to remember that while I strive to be helpful and accurate, I am an AI and may not always provide perfect or flawless responses. It's always a good idea to verify information from multiple sources and use critical thinking when interpreting the outputs."

### 1.3 Our very first Kaggle competition

### Import modules

```{r import modules, warning=FALSE, message=FALSE}
library(caret)
library(glmnet)
library(elasticnet)
library(doParallel)
library(dplyr)
library(lmtest)
```


### The `wmap` dataset

We start by importing the training and testing datasets. The training dataset consists of $675$ angular power spectrum observations (rows) and 3 columns:

- `id` which uniquely identifies each row (necessary to correctly ascertain the "goodness of fit" of our predictions)
- `x` the predictor variable
- `y` the target variable

The `test` data frame contains another $224$  angular power spectrum observations and solely two columns (`id` and `x`).

```{r import data}
train <- read.csv('train.csv')
test <- read.csv('test.csv')
```


### 1.3.1 Equi-spaced knots {#equi}

In this section we aim at fining the "best" $(d, q)$-combination, firstly via plain old vanilla k-fold cross validation, and secondly by implementing Bates' nested cross validation algorithm.

### Vanilla Cross-Validation: K-fold CV

First, we must define some essential **functions** which will make our life much easier when testing our models as the values of $q$, $d$ and other hyper-parameters change.

`remap` maps the feature vector $x$ into a higher-dimensional feature space $X$, a matrix containing $(d+q)$ columns (excluding $g_1(x) = 1$ which is automatically accounted for when using `glmnet`) and `length(x)` rows ($675$ in case of the training set). This function takes in input $d$, $q$, a vector containing the position of the `knots = seq(1/(q+1), 1 - 1/(q+1), 1/(q+1))`, and obviously `x`. The $j^{th}$ column of the feature matrix `X` stores the truncated power basis function $g_{j}(x)$ evaluated at `x` with $2 \leq j \leq d+q+1$.

```{r remap}

remap <- function(d, q, knots, x){
  n = length(x)
  X <- matrix(0, n, d+q)
  for(i in 1:d){
    X[,i] <- (x)**i
  }
  for(j in 1:q){
    idx = d+j
    X[,idx] <- pmax(0, x - rep(knots[j], n))**d
  }
  return(X)
}

```

`bestmodel` applies 5-fold CV to find the "best" combination of $d$ and $q$, namely the one that yields the lowest `rmse` score averaged over the 5 validation folds. The function takes in input `qmax`, `dmax`, `train$x`, `train$y`, `pos` indicating the way we want to position the knots (for now let's suppose the knots are equi-spaced) and `w`, a vector of weights. For each $(d, q)$ couple we 

- `set.seed(123)` inside the function.
- `remap` the feature vector `x` into the feature matrix `X`.
- create the data frame `train_data` containing the $d+q$ feature columns together with the target variable $y$ values.
- choose a tuning grid of $lambda$ values. Specifically, we give `lambda=seq(1e-3, 1, length=100)`, while $alpha$ always equals zero (ridge regression). We experimented with other forms of penalization such as lasso and elastic net which, however, did not drastically improve our model performance while heavily slowing down the search of the best $(d, q)$-combination (running `bestmodel` was slower when not using ridge regression).
- exploit the `train` function of the `caret` package to perform 5-fold CV which spits out the `model` (and hence hyperparameters) yielding the lowest `rmse` score for that choice of `d` and `q`.
- save `lambda`, `rmse` and features matrix `X` into lists.
- return the $(d, q)$-combination which minimizes the validation error.


```{r bestmodel}

bestmodel <- function(qmax=1, dmax=1, train_x, train_y, pos = 'equi', w){
  set.seed(123)
  D <- c(1, 3)
  Q <- 1:qmax
  combinations <- data.frame(expand.grid(D, Q))
  rmse <- c(rep(NA, nrow(combinations)))
  alpha_values <- c(rep(NA, nrow(combinations)))
  lambda_values <- c(rep(NA, nrow(combinations)))
  knots_pos <- list()
  features <- list()
  if(pos == 'equi'){
    knots_pos = lapply(Q, function(q)seq(1/(q+1), 1 - 1/(q+1), 1/(q+1)))
  }
  if (pos == 'quantile') {
    knots_pos = lapply(Q, function(q)
      unname(quantile(train_x,
                      probs = seq(
                        1 / (q + 1), 1 - 1 / (q + 1), 1 / (q + 1)))))
  }
  for(row in 1:nrow(combinations)){
    d = combinations[row, 1]
    q = combinations[row, 2]
    if(pos == 'cluster'){
      dist <- dist(train_y)
      hc <- hclust(dist)
      hc_labels <- cutree(hc, k = q)
      knots <- 
        sort(unlist(lapply(1:q, function(i)
          mean(train_x[hc_labels == i]))))
      knots <- sort(append(knots, 0.26))
      knots_pos[[q]] <- knots
    }
    else{
      knots <- knots_pos[[q]]
    }
    
    X <- remap(d, q, knots, train_x)
    train_data <- data.frame(cbind(X, "y" = train_y))
    train.control <- trainControl(method = "cv", 
                                  number = 5, allowParallel = TRUE)
    my_grid <- expand.grid(alpha = 0, lambda = seq(1e-3, 1, length = 100))
    model <- train(form = y ~. , data = train_data, method = "glmnet",
                   trControl = train.control, metric = "RMSE",
                   tuneGrid = my_grid, weights = w)
    alpha_values[row] <- model$bestTune$alpha
    lambda_values[row] <- model$bestTune$lambda
    rmse[row] <- model$results$RMSE[as.integer(rownames(model$bestTune))]
    features <- append(features, list(X))
  }
  idx_min <- combinations[which.min(rmse),]
  return(list(idx_min, features, rmse, knots_pos, alpha_values, lambda_values))
}

```

`prediction` returns the prediction values `y_pred` necessary to evaluate the performance of the model. Specifically, it

- unpacks the output list of the `bestmodel` function,
- trains a `glmnet` model having as `alpha` and `lambda` parameters the one chosen by the `bestmodel` function
- remaps the test set feature `x` into a feature matrix `Xtest`
- and returns the predictions `y_pred` 

```{r prediction}

prediction <- function(res, train, test){
    # best model hyperparameters
    idx_min = res[[1]]
    d = idx_min$Var1
    q = idx_min$Var2
    X = res[[2]][[as.integer(rownames(idx_min))]]
    rmse = res[[3]][as.integer(rownames(idx_min))]
    knots = res[[4]][[q]]
    alpha = res[[5]][as.integer(rownames(idx_min))]
    lambda = res[[6]][as.integer(rownames(idx_min))]

    # train the model + predict values on test set data
    train_data <- data.frame(cbind(X, "y" = train$y))
    colnames(train_data) <- c(paste0('X', 2:ncol(train_data)-1), 'y')
    mod <- glmnet(X, train$y, alpha = alpha, lambda = lambda,
                  family = "gaussian")
    Xtest = remap(d, q, knots, test$x)
    y_pred <- predict(mod, newx = Xtest)
    
    return(y_pred)
}

```

### How to deal with outliers

Typically, outliers are considered to be values that are below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR`. We define a new data frame `no_outliers` which excludes the $59$ outlier observations, leaving us with $616$ rows. The orange points in the plot below are the outliers.

```{r outliers}

Q1 <- quantile(train$y, .25)
Q3 <- quantile(train$y, .75)
IQR <- unname(Q3 - Q1)

no_outliers <- subset(train, train$y > (Q1 - 1.5*IQR) & train$y < (Q3 + 1.5*IQR))
out <- anti_join(train, no_outliers, by = c('id'='id', 'x'='x', 'y'='y'))

```

```{r outliers plot, echo=FALSE, fig.align='center'}

plot(train$x, train$y, col=adjustcolor("magenta", 0.4), pch = 19,
     main = 'wmap data - outliers detection', xlab = 'x', ylab = 'y')
points(train$x, train$y, col = "magenta4")
points(out$x, out$y, col="orange", pch = 19)
points(out$x, out$y)
text(0.94, 22000, expression(p[1]), cex = 0.8)
text(0.95, 26500, expression(p[2]), cex = 0.8)

```
Outliers can have an impact on _homoscedasticity_, which refers to the assumption that the variability of errors (or residuals) is constant across different levels of the predictor variables. In `R`, we can check for heteroscedasticity among residuals by performing the `Breusch-Pagan Test`. The hypothesis of `BP test` are:

- Null hypothesis: $H_0 =$ residuals are distributed with equal variance (homoscedasticity)
- Alternative hypothesis: $H_1 =$ residuals are distributed with unequal variance (heteroscedasticity)

Here, the output of this `Breusch-Pagan Test` has a high test statistic ($BP=77.559$) and $\text{p-value} < 2.2e^{-16}$. Therefore, we can reject $H_0$ and conclude that the `train` data is indeed heteroscedastic.

```{r heteroscedasticity}

model <- lm(train$y~train$x)
lmtest::bptest(model)

```

Now, how should we deal with _heteroscedasticity_? Instead of just getting rid of all outliers, we can just remove the two farthermost points ($p_1$ and $p_2$ in the plot) and weight less the remaining outliers when training the model. The `weights` vector can be passed directly to the `weights` parameter of `caret`'s `train` function. We set all weights to 1 except for outliers observations whose weights equal 0.5.

```{r outliers dealing}

train <- subset(train, train$y < 20000) # get rid of two farthermost obs
x_train = train$x
y_train = train$y

weights <- c(rep(NA, nrow(train)))
for (row in 1:nrow(train)) {
  weights[row] <- ifelse(train[row, 1] %in% out[,1], 0.5, 1)
}

```

### Run the `bestmodel` function

Let's set `qmax = 40` and `dmax = 3`.

```{r model1}

res <- bestmodel(qmax=40, dmax=3, x_train, y_train, pos='equi', w=weights)

idx_min = res[[1]]
d_best = idx_min$Var1
q_best = idx_min$Var2
X = res[[2]][[as.integer(rownames(idx_min))]]
mse_list = res[[3]]**2
mse_min = mse_list[as.integer(rownames(idx_min))]
knots = res[[4]][[q_best]]
alphas = res[[5]]
lambdas = res[[6]]
alpha_min = res[[5]][as.integer(rownames(idx_min))]
lambda_min = res[[6]][as.integer(rownames(idx_min))]
idx_min

```

The best model according to vanilla cv is the one having `d=1` and `q=40`. That's an awful lot of knots. The cross validation`rmse` equals approximately `2738`, the training set error equals `2591` compared to the _public_ leaderboard test set score of `2729` (excluding the _private_ leaderboard results).

```{r scores model1}

mod <- glmnet(X, train$y, alpha = alpha_min, lambda = lambda_min,
                  family = "gaussian")
y_pred <- predict(mod, newx = X)
paste('rmse on the training set:', round(sqrt(mean((train$y - y_pred)**2)), 2))
paste('k-fold rmse:', round(sqrt(mse_min), 2))

```

Let's have a look at the plot of the fit. We can clearly see that, as we expected from the extreme amount of knots, our model is fitting to noise fluctuations (especially for $x > 0.9$).

```{r plot model1, echo=FALSE, fig.align='center'}

y_pred <- prediction(res, train, test)
y <- data.frame("target" = y_pred)
colnames(y) <- c('target')
df <- data.frame(cbind("id"=test$id, y))
write.csv(df, 'submission12.csv', row.names = F)

plot(train$x, train$y, col=adjustcolor("magenta", 0.4), pch = 19,
     main = 'd = 1 & q = 40', xlab = 'x', ylab = 'y')
points(train$x, train$y)
y_pred <- prediction(res, train, test)
points(test$x, y_pred, col = "orange", pch = 19)
points(test$x, y_pred, col = "orangered2")
abline(v=knots, lty=2)

```

### Bates' et al. Nested CV

Vanilla CV (k-fold) is fine, but it can still lead to overfitting due to not accounting for the correlation between the data points used to train the model and ones belonging to the validation fold. Therefore, the errors computed on the different k folds are also correlated.
Nested CV tries to tackle the correlation issue hence providing a more reliable confidence interval around the errors, allowing us to choose a more generalizable model which also accounts for model complexity. Since we believe that the model chosen by the plain old vanilla cv is too complex, we would like to test the nested cv approach to choose a simpler model which, in turn, should also reduce overfitting.


`inner_crossval` performs k-fold cross validation. Precisely, we

- repeatedly split the training set derived from the outer loop into a new training set (K-1 remaining folds) `in_train` (K-2 folds) and a validation set `val` (1 fold).
- we then train the model on the `in_train` data and fit the model to `val` data.
- finally, we append to `e_in` the vector of residual values `e_temp`.


```{r inner loop}

inner_crossval <- function(d=1, q=1, K=10, knots, out_folds,
                           data, kout, pos='equi'){
    e_in <- c()
    kval <- 1:K
    for(k in kval[kval!=kout]){
        idx <- which(out_folds==k,arr.ind=TRUE)
        val <- data[idx, ]
        temp <- data[-idx, ] # do not consider validation data
        idx_out <- which(out_folds==kout,arr.ind=TRUE)
        in_train <- temp[-idx_out, ] # do not consider holdout data
        X <- remap(d, q, knots, in_train$x)
        model <- glmnet(X, in_train$y, alpha=0)
        Xval <- remap(d, q, knots, val$x)
        y_pred <- predict(model, newx=Xval)
        e_temp <- (y_pred - val$y)**2
        e_in <- append(e_in, e_temp)
    }
    return(list(e_in))
}

```

`outer_crossval` performs the outer loop of the nested cross validation algorithm.

- we repeatedly split the training set `train` into a smaller (K-1 fold) `training` set and a holdout fold (1 fold).
- we apply the `inner_crossval` function to the remaining (K-1) folds and obtain a vector of losses `e_in` which we append to another vector `es`.
- we train a `glmnet` model on the `training` data and evaluate it on the `holdout` data obtaining another vector of losses `e_out`.
- we append `(mean(e_in) - mean(e_out))**2` to the `a_list` vector.
- we append `var(e_out)/nrow(holdout)` to the `b_list` vector.
- finally, we return `es`, `a_list` and `b_list`.


```{r outer loop}

outer_crossval <- function(d=1, q=1, K=10, knots, out_folds,
                           data=data, pos='equi'){
    es <- c()
    a_list <- c()
    b_list <- c()
    for(kout in 1:K){
        idx <- which(out_folds==kout,arr.ind=TRUE)
        holdout <- data[idx, ]  # specify holdout
        training <- data[-idx, ] # specify training
        res_in <- inner_crossval(d, q, K, knots, out_folds, data=data, kout=kout)
        e_in <- res_in[[1]]
        Xtrain <- remap(d, q, knots, training$x)
        Xout <- remap(d, q, knots, holdout$x)
        model <- glmnet(Xtrain, training$y, alpha=0)
        y_pred <- predict(model, newx=Xout)
        e_out <- (y_pred - holdout$y)**2
        a_list <- append(a_list, (mean(e_in) - mean(e_out))**2)
        b_list <- append(b_list, var(e_out)/nrow(holdout))
        es <- append(es, e_in)
    }
    return(list(es, a_list, b_list))
}

```

`nested_cv` function is implements Bates' et al. nested cross validation algorithm.

- we run `outer_crossval` function `R` times, each time randomly sampling from the `train` dataset which is split into `K=10` folds.
- we define the `MSE` as `mean(a_list) - mean(b_list)`.

```{r nested cv}

nested_cv <- function(d=1, q=1, K=10, knots, train, R=5, pos='equi'){
    set.seed(123)
    a_list <- c()
    b_list <- c()
    es <- c()
    for(r in 1:R){
        data <- train[sample(nrow(train)),]
        outer_folds <- cut(seq(1,nrow(data)), breaks=K, labels=FALSE)
        res <- outer_crossval(d, q, K, knots, out_folds=outer_folds,
                              data=data)
        es <- append(es, res[[1]])
        a_list <- append(a_list, res[[2]])
        b_list <- append(b_list, res[[3]])
    }
    MSE <- mean(a_list) - mean(b_list)
    Err <- mean(es)
    return(list(MSE, Err))
}

```

We set `R = 50`, `K = 10` and run the `nested_cv` function. The purpose of this procedure is to estimate the $MSE$ of the "best" model (i.e., the one yielding the lowest `rmse`) to roughly apply the 1-SE rule, which in our case is the 1-$\sqrt{\widehat{MSE}}$ rule.

```{r run nested cv}

ncv_res <- nested_cv(d=d_best, q=q_best, K=10, knots=knots,
                     train, R=50, pos='equi')

```

```{r mse qrt model2}

MSE_sqrt <- sqrt(ncv_res[[1]])
upper_bound <- mse_min + MSE_sqrt
print(paste('MSE Upper bound U:', round(upper_bound, 2),
            '- RMSE Upper bound U:', round(sqrt(upper_bound), 2)))
rmse_list <- round(sqrt(mse_list), 2)

```

`genmodel` selects the _simplest_ model having $Err_{CV} \leq U(Err_{bestmodel})$ where $U(\cdot)$ is the upper bound of the confidence interval $\Bigg( Err_{bestmodel} \pm \sqrt{\widehat{MSE}}\Bigg)$. `Note`: By simplest model we refer to the model having less features ($d+q$).

```{r generalized model}

genmodel <- function(qmax=1, mse_list, upper_bound, pos='equi'){
    D <- c(1, 3)
    Q <- 1:qmax
    combinations <- data.frame(expand.grid(D, Q))
    d_gen = d_best
    q_gen = q_best
    for(model in 1:length(mse_list)){
        d = combinations[model, 1]
        q = combinations[model, 2]
        if(pos == 'cluster'){
          q = q+1
        }
        if((mse_list[model] <= upper_bound) & (d+q <= d_gen+q_gen)){
            d_gen = d
            q_gen = q
        }
    }
    return(list(d_gen, q_gen))
}

```

```{r get generalized model2}

D <- c(1, 3)
Q <- 1:50
combinations <- data.frame(expand.grid(D, Q))
d_gen <- genmodel(50, mse_list, upper_bound)[[1]]
q_gen <- genmodel(50, mse_list, upper_bound)[[2]]
row <- rownames(subset(combinations,
                       combinations$Var1 == d_gen & combinations$Var2 == q_gen))
X <- res[[2]][[as.integer(row)]]
knots <- res[[4]][[q_gen]]
alpha = res[[5]][as.integer(row)]
lambda = res[[6]][as.integer(row)]
print(paste('Chosen d:', d_gen, '& Chosen q:', q_gen))

```

The best model according to he nested cv approach is the one having `d=1` and `q=9`. Clearly, the number of chosen knots is much smaller than the one chosen by the `bestmodel` function. Here, the cross validation `rmse` equals approximately `2738`, the training set error equals `2752` compared to the _public_ leaderboard test set score of `3327` (excluding the _private_ leaderboard results).

```{r scores model2}

mod <- glmnet(X, train$y, alpha = alpha, lambda = lambda,
                  family = "gaussian")
y_pred <- predict(mod, newx = X)
paste('rmse on the training set:', round(sqrt(mean((train$y - y_pred)**2)), 2))
paste('k-fold rmse:', round(sqrt(mse_min), 2))

```

```{r prediction2, echo=FALSE}

train_data <- data.frame(cbind(X, "y" = train$y))
colnames(train_data) <- c(paste0('X', 2:ncol(train_data)-1), 'y')
mod <- glmnet(X, train$y, alpha = alpha, lambda = lambda, family = "gaussian", weights=weights)
Xtest = remap(d_gen, q_gen, knots, test$x)
y_pred <- predict(mod, newx = Xtest)

```

* explain plot.

```{r plot model2, echo=FALSE, fig.align='center'}

y <- data.frame("target" = y_pred)
colnames(y) <- c('target')
df <- data.frame(cbind("id"=test$id, y))
write.csv(df, 'submission13.csv', row.names = F)

plot(train$x, train$y, col=adjustcolor("magenta", 0.4), pch = 19,
     main = 'd = 1 & q = 9', xlab = 'x', ylab = 'y')
points(train$x, train$y)
points(test$x, y_pred, col = "orange", pch = 19)
points(test$x, y_pred, col = "orangered2")
abline(v=knots, lty=2)

```

### 1.3.2 Hierarchical clustered knots

### Vanilla Cross-Validation: K-fold CV

The functions and procedures have already been explained in the previous [section](#equi). The only difference regards the positioning of the knots along `x`. For instance, in the `bestmodel` function we can pass as input `pos=cluster` which chooses the knots positions through hierarchical clustering on the distance matrix `dist` calculated from the `train$y` variable. It then assigns cluster labels `hc_labels` to the observations based on cutting the hierarchical clustering tree into `k = q` clusters. Finally, it calculates the mean of `train$x` for each cluster (knot), sorted in ascending order which corresponds to the positioning of the knot. However, we decided to always add a knot in position $x \approx 0.26$ which is approximately the abscissa of the main bump. 

```{r add knot}

sub <- subset(train, train$x < 0.3)
max_idx <- which.max(sub$y)
sub[max_idx, 2]

```

### Run the `bestmodel` function

Let's set `qmax = 40` and `dmax = 3`.

```{r model3}

res <- bestmodel(qmax=40, dmax=3, x_train, y_train, pos='cluster', w=weights)

idx_min = res[[1]]
d_best = idx_min$Var1
q_best = idx_min$Var2+1
X = res[[2]][[as.integer(rownames(idx_min))]]
mse_list = res[[3]]**2
mse_min = mse_list[as.integer(rownames(idx_min))]
knots = res[[4]][[q_best]]
alphas = res[[5]]
lambdas = res[[6]]
alpha_min = res[[5]][as.integer(rownames(idx_min))]
lambda_min = res[[6]][as.integer(rownames(idx_min))]
paste('d best:', d_best, '& q_best:', q_best)

```

The best model according to vanilla cv is the one having `d=1` and `q=38`. It's still an horrible amount of knots (like in the equispaced case). The cross validation`rmse` equals approximately `2753`, the training set error equals `2511` compared to the _public_ leaderboard test set score of `2926` (excluding the _private_ leaderboard results).

```{r scores model3}

mod <- glmnet(X, train$y, alpha = alpha_min, lambda = lambda_min,
                  family = "gaussian")
y_pred <- predict(mod, newx = X)
paste('rmse on the training set:', round(sqrt(mean((train$y - y_pred)**2)), 2))
paste('k-fold rmse:', round(sqrt(mse_min), 2))

```

Let's have a look at the plot of the fit. We can clearly see that, also in this case, as we expected from the extreme amount of knots, our model is fitting to noise fluctuations (especially for $x > 0.9$). This behaviour is accentuated in the clustered case as the majority of knots are situated in the noiser portion of data.

```{r plot model3, echo=FALSE, fig.align='center'}

y_pred <- prediction(res, train, test)
y <- data.frame("target" = y_pred)
colnames(y) <- c('target')
df <- data.frame(cbind("id"=test$id, y))
write.csv(df, 'submission15.csv', row.names = F)

plot(train$x, train$y, col=adjustcolor("magenta", 0.4), pch = 19,
     main = 'd = 1 & q = 38', xlab = 'x', ylab = 'y')
points(train$x, train$y)
y_pred <- prediction(res, train, test)
points(test$x, y_pred, col = "orange", pch = 19)
points(test$x, y_pred, col = "orangered2")
abline(v=knots, lty=2)

```

### Bates' et al. Nested CV

Also in the clustered-case we apply nested cross validation to find a simpler model. We set `R = 50`, `K = 10` and run the `nested_cv` function.

```{r run nested cv cluster knots}

ncv_res <- nested_cv(d=d_best, q=q_best, K=10, knots=knots, train, R=50)

```

```{r mse qrt model4}

MSE_sqrt <- sqrt(ncv_res[[1]])
upper_bound <- mse_min + MSE_sqrt
print(paste('MSE Upper bound U:', round(upper_bound, 2),
            '- RMSE Upper bound U:', round(sqrt(upper_bound), 2)))
rmse_list <- round(sqrt(mse_list), 2)

```
```{r get generalized model4}

D <- c(1, 3)
Q <- 1:50
combinations <- data.frame(expand.grid(D, Q))
gen_res <- genmodel(50, mse_list, upper_bound, pos='cluster')
d_gen <- gen_res[[1]]
q_gen <- gen_res[[2]]
row <- rownames(subset(combinations,
                       combinations$Var1 == d_gen & combinations$Var2 == q_gen))
X <- res[[2]][[as.integer(row)]]
knots <- res[[4]][[q_gen]]
alpha = res[[5]][as.integer(row)]
lambda = res[[6]][as.integer(row)]
print(paste('Chosen d:', d_gen, '& Chosen q:', q_gen))

```

The best model according to he nested cv approach is the one having `d=1` and `q=5`. Clearly, the number of chosen knots is much smaller than the one chosen by the `bestmodel` function. Here, the cross validation `rmse` equals approximately `2753`, the training set error equals `2750` compared to the _public_ leaderboard test set score of `3327` (excluding the _private_ leaderboard results).

```{r scores model4}

mod <- glmnet(X, train$y, alpha = alpha, lambda = lambda,
                  family = "gaussian")
y_pred <- predict(mod, newx = X)
paste('rmse on the training set:', round(sqrt(mean((train$y - y_pred)**2)), 2))
paste('k-fold rmse:', round(sqrt(mse_min), 2))

```

```{r prediction4, echo=FALSE}

train_data <- data.frame(cbind(X, "y" = train$y))
colnames(train_data) <- c(paste0('X', 2:ncol(train_data)-1), 'y')
mod <- glmnet(X, train$y, alpha = alpha, lambda = lambda, family = "gaussian", weights=weights)
Xtest = remap(d_gen, q_gen, knots, test$x)
y_pred <- predict(mod, newx = Xtest)

```

* explain plot.

```{r plot model4, echo=FALSE, fig.align='center'}

y <- data.frame("target" = y_pred)
colnames(y) <- c('target')
df <- data.frame(cbind("id"=test$id, y))
write.csv(df, 'submission16.csv', row.names = F)

plot(train$x, train$y, col=adjustcolor("magenta", 0.4), pch = 19,
     main = 'd = 1 & q = 5', xlab = 'x', ylab = 'y')
points(train$x, train$y)
points(test$x, y_pred, col = "orange", pch = 19)
points(test$x, y_pred, col = "orangered2")
abline(v=knots, lty=2)

```


### Conclusion

\newpage

# Part II

## Question 1

From our point of view the emergence of language models such as Chat Gpt and their massive diffusion are leading to the arising of a whole series of problems related to their use, ranging from ethics to information, to privacy and all the potential effects of its further and rapid development. 
First of all since these models are dependent on training data, if these have discriminatory tendencies towards certain
categories, there could be questionable results that however reflect human social issues of the present. Nevertheless this further consequences that this could have at the level of AI could not be ignore. Therefore, it would be necessary to regulate carefully the data selection to train this models to prevent it from contributing to the persistence of such discriminations.
An example of this, related more in general to AI is [The documentary Coded Bias](https://www.codedbias.com), which discloses the observations made by MIT researcher Joy Buolamwini who observed how many facial recognition technologies do not accurately detect darker-skinned faces or classify the faces of women, 'inheriting' in some way human prejudices.
Considering that prejudices are strictly interconnected to fake news, this aspect is closely linked to the issue of information in the sense that generative models risk leading to the creation of false information, creating in some sense false knowledge. In particular, GPT being driven by web data, which is the realm of false information in itself, this aspect is extremely pronounced.
On the other hand, the idea that this data can be controlled and filtered could be 
a controversial aspect when it leads to the generation from information polarized by ideologies. Therefore, in order to maintain a democratic aspect, it would be necessary to set limits and trying to keep neutrality as much as possible but that is not as simple as it could be seen. Indeed, the possibility of restricting access to these tools by the entire world population is also a very controversial topic.

Another aspect to consider is the role of these bots in education and among the young; while it can be a tool to help you be more productive and concentrate on other tasks,  the speed and ease with which they can provide text customized to the to the question asked inevitably leads to greater possibilities
of cheating, preventing the student from applying himself from scratch in learning activities.
Moreover, from a future perspective, the presence of increasingly complex interactive models could threaten or in any case bring about a change in the role of teachers. More in general it is a factor of concern that these bots it may replace other kind human activities by decreasing the employment of human beings. This irrational fear is quite widespread, but it's not a new reaction, since is a constant with respect to innovation that already was present in the past and influenced media with the creation of worlds in which human beings were replaced by AI's or the seizure of power by armies of robots. Beyond dystopic imaginaries, it is concretely necessary to question the capabilities of these tools to go beyond training data (as highlighted in the same GPT4 technical report) and develop further abilities which were not predictable. In conclusion, we think it is necessary to try to understand the potential of what is being created through the knowledge we have, without interfering with its development, but we believe it is more urgent to regulate it in the present, considering the effects that the massive diffusion of these tools could have and also the improper uses that could be made of it.

\
\
\
\
\

Our job here is done :)

\
\

```{r, fig.align='center', out.width='450px', out.height='200px', echo=FALSE}
knitr::include_graphics("final.jpeg")
```





